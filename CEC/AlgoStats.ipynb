{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import ctypes\n",
    "import sys\n",
    "import time\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test the type of distribution of data for any distribution\n",
    "from scipy.stats import kstest\n",
    "# Test for the Heteroscedasticity\n",
    "from scipy.stats import levene\n",
    "# Test with nonparametric test for comparing of two\n",
    "from scipy.stats import wilcoxon\n",
    "# Test with nonparametric test for comparing of three or more\n",
    "from scipy.stats import friedmanchisquare\n",
    "# probability plot\n",
    "from scipy.stats import probplot\n",
    "\n",
    "np.set_printoptions(linewidth=100000000, formatter={'all': lambda x: str(x)})\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from run_cec import MinMB, fillprototype\n",
    "\n",
    "CECv = 2017\n",
    "cec_dll = ctypes.cdll.LoadLibrary('cec%d/cec%d.so' % (CECv, CECv))\n",
    "prototype = ctypes.CFUNCTYPE(    \n",
    "    ctypes.c_double,                \n",
    "    ctypes.POINTER(ctypes.c_double),                \n",
    "    ctypes.c_int,\n",
    "    ctypes.c_int\n",
    ")\n",
    "run_fun = prototype(('runtest', cec_dll))\n",
    "\n",
    "from NiaPy.algorithms import AlgorithmUtility\n",
    "from NiaPy.task import StoppingTask, Utility\n",
    "from NiaPy.util import reflectRepair\n",
    "\n",
    "def testRunTest(a, d, bench, **args):\n",
    "   task = StoppingTask(D=d, nFES=d * 1e4, benchmark=bench, **args)\n",
    "   start_time = time.time()\n",
    "   best = a(task)\n",
    "   return time.time() - start_time, best\n",
    "\n",
    "def testOne(x=0.55):\n",
    "   for i in range(10 ** 6): \n",
    "      x = x + x\n",
    "      x = x / 2\n",
    "      x = x * x\n",
    "      x = np.sqrt(x)\n",
    "      x = np.log(x)\n",
    "      x = np.exp(x)\n",
    "      x = x / (x + 2)\n",
    "\n",
    "def testTwo(d):\n",
    "   for i in range(2 * 10 ** 5):\n",
    "      x = np.random.uniform(-100, 100, d)\n",
    "      run_fun(x.ctypes.data_as(ctypes.POINTER(ctypes.c_double)), ctypes.c_int(d), ctypes.c_int(18))\n",
    "\n",
    "def testThreeCec(a, d, fnum=1): return testRunTest(a, d, MinMB(cec_dll.runtest, fnum=fnum))\n",
    "\n",
    "def testThreeBasic(a, d, fnum=1):\n",
    "   mapper = {\n",
    "      1:'bentcigar',\n",
    "      2:'',\n",
    "      3:'zakharov',\n",
    "      4:'rosenbrock',\n",
    "      5:'rastrigin',\n",
    "      6:'',\n",
    "      10:'swefel'\n",
    "   }\n",
    "   return testRunTest(a, d, mapper[fnum])\n",
    "\n",
    "def t_fun_cec(a, d, fnum, q, runs_no):\n",
    "   for _ in range(runs_no): q.put(testThreeCec(a, d, fnum)[1][1])\n",
    "\t  \n",
    "def t_fun_basic(a, d, fnum, q, runs_no):\n",
    "   for _ in range(runs_no): q.put(testThreeBasic(a, d, fnum)[1][1])\n",
    "   \n",
    "def runThread(a, d, fnum=1, thread_fun=t_fun_cec, thread_no=24, runs_no=35, seed=1, **a_args):\n",
    "   autil = AlgorithmUtility()\n",
    "   ts, qs, r, runs, runs_sum = [], [], [], [0] * thread_no, 0\n",
    "   i = 0\n",
    "   while runs_sum < runs_no:\n",
    "      runs[i] += 1\n",
    "      runs_sum += 1\n",
    "      i = i + 1 if i + 1 < len(runs) else 0\n",
    "   for i in range(thread_no):\n",
    "      if runs[i] == 0: continue\n",
    "      q = Queue(runs[i])\n",
    "      t = Process(target=thread_fun, args=(autil.get_algorithm(a)(seed=seed + i, **a_args), d, fnum, q, runs[i]))\n",
    "      t.start()\n",
    "      ts.append(t), qs.append(q)\n",
    "   for t in ts: t.join()\n",
    "   for q in qs: \n",
    "      while not q.empty(): r.append(q.get())\n",
    "   return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dims, algosNames, alpha = [10, 30, 50], ['BA', 'ABA', 'SABA', 'HBA', 'HSABA'], 0.05\n",
    "seed = 1\n",
    "algs = {\n",
    "   'BA': {},\n",
    "   'HBA': {},\n",
    "   'SABA': {},\n",
    "   'HSABA': {\n",
    "      'NP': 100,\n",
    "      'A': 0.5,\n",
    "      'F': 0.5,\n",
    "      'CR': 0.9,\n",
    "\t  'r': 0.01,\n",
    "      'Qmax': 1,\n",
    "      'Qmin': -1.2\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEC 2017 algorithm speed test\n",
    "Example of time execution speed of algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "testOne()\n",
    "t0 = (time.time() - start_time)\n",
    "print(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = []\n",
    "for d in dims:\n",
    "   start_time = time.time()\n",
    "   testTwo(d)\n",
    "   t1.append(time.time() - start_time)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on basic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autil = AlgorithmUtility()\n",
    "res, f = {}, 1\n",
    "for k, v in algs.items():\n",
    "   a = autil.get_algorithm(k)(seed=seed + 1, **v)\n",
    "   r = testThreeBasic(a, d, f)\n",
    "   res[k] = [r[0], r[1][1]]\n",
    "display('Func %d' % f)\n",
    "display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on CEC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autil = AlgorithmUtility()\n",
    "for f in range(1):\n",
    "   res = {}\n",
    "   for k, v in algs.items():\n",
    "      a = autil.get_algorithm(k)(seed=seed + 1, **v)\n",
    "      r = testThreeCec(a, d, f + 1)\n",
    "      res[k] = [r[0], r[1][1]]\n",
    "   display('Func %d' % (f + 1))\n",
    "   display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on multiple threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on basic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, d, f = {}, 10, 1\n",
    "for k, v in algs.items(): res[k] = runThread(k, d, f, thread_fun=t_fun_basic, **v)\n",
    "display('Func %d' % f)\n",
    "display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on CEC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, d, f = {}, 10, 1\n",
    "for k, v in algs.items(): res[k] = runThread(k, d, f, thread_fun=t_fun_cec, **v)\n",
    "display('Func %d' % f)\n",
    "display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEC 2017 statistic\n",
    "For getting the data need to perform statistic test we used: `for i in {1..30}; do python run_cec.py -c 17 -o T -rn 50 -a DE -D 10 -f $i -seed {1000..1050}; done`.\n",
    "The example shows how to run DE algorithm on all benchmark functions that have problem dimensionality set to 10.\n",
    "DE algorithm runs 50 times on each benchmark functions with seed in range from 1000 to 1050.\n",
    "Every algorithm run has it's own seed.\n",
    "We used `-o T` for generating the output.\n",
    "\n",
    "## Example of multiple runs on one problem\n",
    "* dim $\\in \\{10, 30, 50\\}$\n",
    "* fnum $\\in \\{1, \\cdots , 30\\}$\n",
    "* algos $\\in$ `{dynNpMsjDE, BBFWA, DE, jDE, SCA, ES(1+1), ES(m+1), ASO, BA, dynFWAG}`\n",
    "\n",
    "## Load data\n",
    "We will create a new variable `data`, that holds the optization results for 50 runs, 30 functions and specified algorithms.\n",
    "Now first index presents algorithm, second index presents function and third index presents the value of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res, d = {}, 10\n",
    "for k, v in algs.items():\n",
    "   print('START algor run for %s' % k)\n",
    "   for f in range(1, 31):\n",
    "      print('START function %d run' % f)\n",
    "      tmp = runThread(k, d, f, thread_fun=t_fun_cec, **v)\n",
    "      if res.get(k, None) is None: res[k] = [tmp]\n",
    "      else: res[k].append(tmp)\n",
    "      print('END function run')\n",
    "   print('END algor run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "Now we transform the presentation of data in our new array called `ndata` that has normalized data.\n",
    "Now first index presents function, second index presents algorithm and third index presents the value of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = np.asanyarray([[(data[j, i] - np.min(data[j, i])) / (np.max(data[j, i]) - np.min(data[j, i])) for j in range(len(algos))] for i in range(30)])\n",
    "ndata[np.isnan(ndata)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for normal distribution\n",
    "[Kolmogorov–Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) test: Tests whether a sample is drawn from a given distribution, or whether two samples are drawn from the same distribution.\n",
    "\n",
    "If $\\text{p-value} > \\alpha$ then values belong to selected distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(algos):\n",
    "   tmp = ''\n",
    "   for j in range(30):\n",
    "      s = kstest(ndata[j, i], 'norm')\n",
    "      print('%10s on %2d.: %.3E %.3E' % (a, j + 1, s[0], s[1]))\n",
    "      tmp += '%.3E & ' % s[1]\n",
    "   print(tmp, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Plot for one algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "   fig = plt.figure()\n",
    "   ax = fig.add_subplot(111)\n",
    "   probplot(ndata[i, 0], plot=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Heteroscedasticity\n",
    "The Levene test tests the null hypothesis that all input samples are from populations with equal variances. Levene’s test is an alternative to Bartlett’s test bartlett in the case where there are significant deviations from normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ''\n",
    "for j in range(30):\n",
    "   s = levene(*ndata[j, :], center='mean')\n",
    "   print('%2d.: %.3E %.3E' % (j + 1, s[0], s[1]))\n",
    "   tmp += '%.3E & ' % s[1]\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Wilcoxon test\n",
    "[Wilcoxon signed-rank](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) test: tests whether matched pair samples are drawn from populations with different mean ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(30):\n",
    "   tmp = ''\n",
    "   print ('f%2d' % (j + 1))\n",
    "   for i, a in enumerate(algos):\n",
    "      s = wilcoxon(ndata[j, 0], ndata[j, i])\n",
    "      print ('%10s vs. %10s on %2d. fun: %.3E %.3E' % (algos[0], a, j, s[0], s[1]))\n",
    "      tmp += '%.3E & ' % s[1]\n",
    "   print (tmp, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Friedman test\n",
    "[Friedman two-way analysis of variance by ranks](https://en.wikipedia.org/wiki/Friedman_test): tests whether k treatments in randomized block designs have identical effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(30):\n",
    "   print (algos, ' on ', j + 1, ' fun: ', friedmanchisquare(*ndata[j, :]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
