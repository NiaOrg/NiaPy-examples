{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test the type of distribution of data for any distribution\n",
    "from scipy.stats import kstest\n",
    "# Test for the Heteroscedasticity\n",
    "from scipy.stats import levene\n",
    "# Test with nonparametric test for comparing of two\n",
    "from scipy.stats import wilcoxon\n",
    "# Test with nonparametric test for comparing of three or more\n",
    "from scipy.stats import friedmanchisquare\n",
    "# probability plot\n",
    "from scipy.stats import probplot\n",
    "\n",
    "np.set_printoptions(linewidth=100000000, formatter={'all': lambda x: str(x)})\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims, algosNames, alpha = [10, 30, 50], ['BA', 'ABA', 'SABA', 'HBA', 'HSABA'], 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEC 2017 algorithm speed test\n",
    "Example of time execution speed of algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('cec2017')\n",
    "from cec2017 import run_fun\n",
    "from run_cec import MinMB\n",
    "\n",
    "from NiaPy.algorithms import AlgorithmUtility\n",
    "from NiaPy.task import StoppingTask, Utility\n",
    "from NiaPy.util import reflectRepair\n",
    "from NiaPy.algorithms.basic import BatAlgorithm\n",
    "from NiaPy.algorithms.modified import HybridBatAlgorithm, AdaptiveBatAlgorithm, SelfAdaptiveBatAlgorithm, HybridSelfAdaptiveBatAlgorithm\n",
    "\n",
    "def testOne(x=0.55):\n",
    "   for i in range(10 ** 6): \n",
    "      x = x + x\n",
    "      x = x / 2\n",
    "      x = x * x\n",
    "      x = np.sqrt(x)\n",
    "      x = np.log(x)\n",
    "      x = np.exp(x)\n",
    "      x = x / (x + 2)\n",
    "\n",
    "def testTwo(d):\n",
    "   for i in range(2 * 10 ** 5):\n",
    "      x = np.random.uniform(-100, 100, d)\n",
    "      run_fun(x, 18)\n",
    "\n",
    "def testThreeCec(a, d, fnum=1):\n",
    "   task = StoppingTask(D=d, nFES=d * 1e4, benchmark=MinMB(run_fun, fnum=fnum))\n",
    "   start_time = time.time()\n",
    "   best = a(task)\n",
    "   return time.time() - start_time, best\n",
    "\n",
    "def testThreeBasic(a, d, fnum=1):\n",
    "   mapper = {\n",
    "      1:'bentcigar',\n",
    "      2:'',\n",
    "      3:'zakharov',\n",
    "      4:'rosenbrock',\n",
    "      5:'rastrigin',\n",
    "      6:'',\n",
    "      10:'swefel'\n",
    "   }\n",
    "   task = StoppingTask(D=d, nFES=d * 1e4, benchmark=mapper[fnum], frepair=reflectRepair)\n",
    "   start_time = time.time()\n",
    "   best = a(task)\n",
    "   return time.time() - start_time, best\n",
    "\n",
    "def t_fun_cec(a, d, fnum, q, runs_no):\n",
    "   for _ in range(runs_no): q.put(testThreeCec(a, d, fnum[1][1]))\n",
    "\t  \n",
    "def t_fun_basic(a, d, fnum, q, runs_no):\n",
    "   for _ in range(runs_no): q.put(testThreeBasic(a, d, fnum)[1][1])\n",
    "   \n",
    "def runThread(a, d, fnum=1, thread_fun=t_fun_cec, thread_no=24, runs_no=35, seed=1, **a_args):\n",
    "   autil = AlgorithmUtility()\n",
    "   ts, qs, r, runs, runs_sum = [], [], [], int(np.ceil(runs_no / thread_no)), 0\n",
    "   for i in range(thread_no):\n",
    "      no_r = runs if (runs_sum + runs) <= runs_no else (runs_no - runs_sum if runs_sum < runs_no else 0)\n",
    "      runs_sum += runs\n",
    "      q = Queue(no_r)\n",
    "      t = Process(target=thread_fun, args=(autil.get_algorithm(a)(seed=seed + i, **a_args), d, fnum, q, no_r))\n",
    "      t.start()\n",
    "      ts.append(t), qs.append(q)\n",
    "   for t in ts: t.join()\n",
    "   for q in qs: r.append(q.get())\n",
    "   return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "algs = {\n",
    "   'BA': {},\n",
    "   'HBA': {},\n",
    "   'SABA': {},\n",
    "   'HSABA': {\n",
    "      'NP': 100,\n",
    "      'A': 0.5,\n",
    "      'F': 0.5,\n",
    "      'CR': 0.9,\n",
    "\t  'r': 0.01\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "testOne()\n",
    "t0 = (time.time() - start_time)\n",
    "print(t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = []\n",
    "for d in dims:\n",
    "   start_time = time.time()\n",
    "   testTwo(d)\n",
    "   t1.append(time.time() - start_time)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on basic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autil = AlgorithmUtility()\n",
    "res, f = {}, 1\n",
    "for k, v in algs.items():\n",
    "   a = autil.get_algorithm(k)(seed=seed + 1, **v)\n",
    "   r = testThreeBasic(a, d, f)\n",
    "   res[k] = [r[0], r[1][1]]\n",
    "display('Func %d' % f)\n",
    "display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on CEC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autil = AlgorithmUtility()\n",
    "for f in range(1):\n",
    "   res = {}\n",
    "   for k, v in algs.items():\n",
    "      a = autil.get_algorithm(k)(seed=seed + 1, **v)\n",
    "      r = testThreeCec(a, d, f + 1)\n",
    "      res[k] = [r[0], r[1][1]]\n",
    "   display('Func %d' % (f + 1))\n",
    "   display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on multiple threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on basic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, d, f = {}, 10, 1\n",
    "for k, v in algs.items():\n",
    "   tmp = runThread(k, d, f, thread_fun=t_fun_basic, **v)\n",
    "   res[k] = tmp if res.get(k, None) is None else res.extend(tmp)\n",
    "display('Func %d' % f)\n",
    "display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on CEC function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, d, f = {}, 10, 1\n",
    "for k, v in algs.items(): res[k] = runThread(k, d, f, thread_fun=t_fun_cec, **v)\n",
    "display('Func %d' % f)\n",
    "display(pd.DataFrame.from_dict(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CEC 2017 statistic\n",
    "For getting the data need to perform statistic test we used: `for i in {1..30}; do python run_cec.py -c 17 -o T -rn 50 -a DE -D 10 -f $i -seed {1000..1050}; done`.\n",
    "The example shows how to run DE algorithm on all benchmark functions that have problem dimensionality set to 10.\n",
    "DE algorithm runs 50 times on each benchmark functions with seed in range from 1000 to 1050.\n",
    "Every algorithm run has it's own seed.\n",
    "We used `-o T` for generating the output.\n",
    "\n",
    "## Example of multiple runs on one problem\n",
    "* dim $\\in \\{10, 30, 50\\}$\n",
    "* fnum $\\in \\{1, \\cdots , 30\\}$\n",
    "* algos $\\in$ `{dynNpMsjDE, BBFWA, DE, jDE, SCA, ES(1+1), ES(m+1), ASO, BA, dynFWAG}`\n",
    "\n",
    "## Load data\n",
    "We will create a new variable `data`, that holds the optization results for 50 runs, 30 functions and specified algorithms.\n",
    "Now first index presents algorithm, second index presents function and third index presents the value of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dim, data = dims[0], []\n",
    "data = np.asanyarray([[np.loadtxt('data/%s_%d_%d_v' % (a, fnum, dim)) for fnum in range(1, 31)] for a in algos])\n",
    "# Get basic statistics\n",
    "vals = []\n",
    "for fnum in range(30):\n",
    "   tmp = []\n",
    "   print ('\\nfun_num: %d' % (fnum + 1))\n",
    "   for i, a in enumerate(algos):\n",
    "      d = data[i, fnum] = data[i, fnum] - (fnum + 1) * 100\n",
    "      print ('%10s:\\tmin: %.3E \\tmean: %.3E\\tstd: %.3E' % (a, np.min(d), np.mean(d), np.std(d)))\n",
    "      tmp.append((np.min(d), np.mean(d), np.std(d)))\n",
    "   vals.append(tmp)\n",
    "vals = np.asanyarray(vals)\n",
    "# Get best values for basic statistics\n",
    "imin, imean, istd = [], [], []\n",
    "for fnum in range(30):\n",
    "   imin.append(np.argmin([vals[fnum, i, 0] for i in range(len(algos))]))\n",
    "   imean.append(np.argmin([vals[fnum, i, 1] for i in range(len(algos))]))\n",
    "   istd.append(np.argmin([vals[fnum, i, 2] for i in range(len(algos))]))\n",
    "# Generate table entrys for latex\n",
    "out = ''\n",
    "for i in range(len(algos)):\n",
    "   for fnum in range(30): out += ('%.3E' if i != imin[fnum] else '\\\\textbf{%.3E}') % vals[fnum, i, 0] + ' & ' + ('%.3E' if i != imean[fnum] else '\\\\textbf{%.3E}') % vals[fnum, i, 1] + ' & ' + ('%.3E' if i != istd[fnum] else '\\\\textbf{%.3E}') % vals[fnum, i, 2] + ' \\\\\\\\ \\n'\n",
    "   out += '\\n'\n",
    "print ('\\n', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize the data\n",
    "Now we transform the presentation of data in our new array called `ndata` that has normalized data.\n",
    "Now first index presents function, second index presents algorithm and third index presents the value of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata = np.asanyarray([[(data[j, i] - np.min(data[j, i])) / (np.max(data[j, i]) - np.min(data[j, i])) for j in range(len(algos))] for i in range(30)])\n",
    "ndata[np.isnan(ndata)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for normal distribution\n",
    "[Kolmogorov–Smirnov](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) test: Tests whether a sample is drawn from a given distribution, or whether two samples are drawn from the same distribution.\n",
    "\n",
    "If $\\text{p-value} > \\alpha$ then values belong to selected distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(algos):\n",
    "   tmp = ''\n",
    "   for j in range(30):\n",
    "      s = kstest(ndata[j, i], 'norm')\n",
    "      print('%10s on %2d.: %.3E %.3E' % (a, j + 1, s[0], s[1]))\n",
    "      tmp += '%.3E & ' % s[1]\n",
    "   print(tmp, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Plot for one algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(30):\n",
    "   fig = plt.figure()\n",
    "   ax = fig.add_subplot(111)\n",
    "   probplot(ndata[i, 0], plot=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for Heteroscedasticity\n",
    "The Levene test tests the null hypothesis that all input samples are from populations with equal variances. Levene’s test is an alternative to Bartlett’s test bartlett in the case where there are significant deviations from normality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ''\n",
    "for j in range(30):\n",
    "   s = levene(*ndata[j, :], center='mean')\n",
    "   print('%2d.: %.3E %.3E' % (j + 1, s[0], s[1]))\n",
    "   tmp += '%.3E & ' % s[1]\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Wilcoxon test\n",
    "[Wilcoxon signed-rank](https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test) test: tests whether matched pair samples are drawn from populations with different mean ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(30):\n",
    "   tmp = ''\n",
    "   print ('f%2d' % (j + 1))\n",
    "   for i, a in enumerate(algos):\n",
    "      s = wilcoxon(ndata[j, 0], ndata[j, i])\n",
    "      print ('%10s vs. %10s on %2d. fun: %.3E %.3E' % (algos[0], a, j, s[0], s[1]))\n",
    "      tmp += '%.3E & ' % s[1]\n",
    "   print (tmp, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Friedman test\n",
    "[Friedman two-way analysis of variance by ranks](https://en.wikipedia.org/wiki/Friedman_test): tests whether k treatments in randomized block designs have identical effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(30):\n",
    "   print (algos, ' on ', j + 1, ' fun: ', friedmanchisquare(*ndata[j, :]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
